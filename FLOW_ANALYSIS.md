# ğŸ” Cross-Attentionå®ç°æµç¨‹åˆ†æ

## å®Œæ•´è®­ç»ƒæµç¨‹

### 1. æ•°æ®åŠ è½½
```
Triptych from dataset: [B, 3, 512, 1536]
  = [visible_img | target_img | semantic_img]
    [0:512]      [512:1024]    [1024:1536]
```

### 2. ç¼–ç é˜¶æ®µ (åœ¨no_gradå†…)

```python
with torch.no_grad():
    # 2.1 ç¼–ç æ–‡æœ¬
    prompt_embeds, pooled_prompt_embeds, text_ids = encode_prompt(...)

    # 2.2 åˆ†ç¦»å›¾åƒ
    visible_img = triptych[:, :, :, 0:512]      # [B, 3, 512, 512]
    target_img = triptych[:, :, :, 512:1024]    # [B, 3, 512, 512]
    semantic_img = triptych[:, :, :, 1024:1536] # [B, 3, 512, 512]

    # 2.3 æ„é€ diptychå¹¶ç¼–ç 
    diptych = cat([visible_img, target_img], dim=-1)  # [B, 3, 512, 1024]

    x_0, x_cond, img_ids = encode_images_fill(diptych, mask)
    # x_0: [B, 2048, 64]    - target latent (ground truth)
    # x_cond: [B, 2048, 320] - masked condition (64 image + 256 mask)

    # 2.4 å‡†å¤‡å™ªå£°å’Œæ’å€¼
    x_1 = randn_like(x_0)  # [B, 2048, 64] - pure noise
    t = random_timestep     # [B]
    x_t = (1-t) * x_0 + t * x_1  # [B, 2048, 64] - noisy latent
```

### 3. è¯­ä¹‰ç¼–ç  (åœ¨no_gradå¤–,ä½†VAEä»ç„¶frozen)

```python
# åœ¨with torch.no_grad()å—å¤–
if semantic_mode == 'cross_attention' and semantic_img is not None:
    with torch.no_grad():
        # VAEæ˜¯frozençš„,æ‰€ä»¥ä»ç„¶no_grad
        semantic_tokens, _ = encode_images(semantic_img)
        # semantic_tokens: [B, 2048, 64]
```

**å…³é”®**: semantic_tokensè™½ç„¶åœ¨no_gradå†…ç”Ÿæˆ,ä½†:
- VAEæœ¬èº«æ˜¯frozençš„,ä¸éœ€è¦æ¢¯åº¦
- Cross-attentionçš„æ¢¯åº¦å¯ä»¥æ­£å¸¸å›ä¼ 
- è¿™å°±åƒä¸€ä¸ªdetachedçš„condition signal

### 4. Cross-Attentionæ³¨å…¥

```python
# åˆå§‹hidden_states
hidden_states = cat([x_t, x_cond], dim=2)  # [B, 2048, 384]

# åº”ç”¨cross-attention (æœ‰æ¢¯åº¦!)
if semantic_cross_attn is not None and semantic_tokens is not None:
    x_t_enhanced = semantic_cross_attn(x_t, semantic_tokens)
    # x_t: [B, 2048, 64] (query)
    # semantic_tokens: [B, 2048, 64] (key, value)
    # x_t_enhanced: [B, 2048, 64] (output)

    # é‡æ–°ç»„åˆ
    hidden_states = cat([x_t_enhanced, x_cond], dim=2)  # [B, 2048, 384]
```

**Cross-Attentionå†…éƒ¨**:
```python
class SemanticCrossAttention:
    def forward(image_feat, semantic_feat):
        # Normalize
        image_norm = LayerNorm(image_feat)
        semantic_norm = LayerNorm(semantic_feat)

        # Project
        Q = Linear_q(image_norm)    # [B, 2048, 64]
        K = Linear_k(semantic_norm) # [B, 2048, 64]
        V = Linear_v(semantic_norm) # [B, 2048, 64]

        # Multi-head attention (8 heads)
        Q = reshape(Q, [B, num_heads, 2048, head_dim])  # [B, 8, 2048, 8]
        K = reshape(K, [B, num_heads, 2048, head_dim])
        V = reshape(V, [B, num_heads, 2048, head_dim])

        # Attention
        scores = Q @ K.T * scale  # [B, 8, 2048, 2048]
        attn = softmax(scores)
        out = attn @ V            # [B, 8, 2048, 8]

        # Reshape and project
        out = reshape(out, [B, 2048, 64])
        out = Linear_out(out)

        # Residual with learnable scale
        return image_feat + scale * out  # scaleåˆå§‹åŒ–ä¸º0
```

### 5. Transformer Forward

```python
transformer_out = transformer(
    hidden_states=hidden_states,  # [B, 2048, 384] (x_t_enhanced + x_cond)
    timestep=t,
    guidance=guidance,
    pooled_projections=pooled_prompt_embeds,
    encoder_hidden_states=prompt_embeds,
    txt_ids=text_ids,
    img_ids=img_ids,
)

pred = transformer_out[0]  # [B, 2048, 64] - é¢„æµ‹çš„é€Ÿåº¦åœº
```

### 6. Lossè®¡ç®—

```python
# Flow Matchingè®­ç»ƒç›®æ ‡
target = x_1 - x_0  # [B, 2048, 64] - é€Ÿåº¦åœºground truth
loss = MSE(pred, target)
```

---

## æ¢¯åº¦æµåˆ†æ

### âœ… æœ‰æ¢¯åº¦çš„éƒ¨åˆ†:
1. **Cross-Attentionæ¨¡å—** (semantic_cross_attn)
   - Q, K, V projections
   - Output projection
   - LayerNorm parameters
   - Scale parameter

2. **Transformer LoRAå±‚**
   - æ‰€æœ‰LoRA adapter weights

### âŒ æ— æ¢¯åº¦çš„éƒ¨åˆ†(frozen):
1. VAE encoder/decoder
2. CLIP text encoder
3. T5 text encoder
4. Transformer backbone (é™¤äº†LoRA)

### æ¢¯åº¦å›ä¼ è·¯å¾„:
```
Loss (MSE)
  â†“
pred (transformer output)
  â†“
transformer blocks + LoRA
  â†“
hidden_states [x_t_enhanced | x_cond]
  â†“
x_t_enhanced (cross-attention output)
  â†“
semantic_cross_attn (Q,K,V projections + scale)
  â†“
x_t (detached, å› ä¸ºæ¥è‡ªno_grad)
semantic_tokens (detached, å› ä¸ºæ¥è‡ªno_grad)
```

**å…³é”®**: è™½ç„¶x_tå’Œsemantic_tokensæ˜¯detachedçš„,ä½†cross-attentionçš„**å‚æ•°**ä»ç„¶å¯ä»¥å­¦ä¹ !

è¿™å°±åƒ:
```python
x = torch.randn(10).detach()  # æ— æ¢¯åº¦
w = nn.Parameter(torch.randn(10))  # æœ‰æ¢¯åº¦
y = (x * w).sum()  # yå¯¹wæœ‰æ¢¯åº¦!
y.backward()  # w.gradä¸ä¸ºNone
```

---

## ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡æ˜¯åˆç†çš„?

### 1. è®­ç»ƒç›®æ ‡ä¸€è‡´æ€§
- Transformerçš„ä»»åŠ¡: é¢„æµ‹é€Ÿåº¦åœº `v = x_1 - x_0`
- Cross-attentionçš„ä½œç”¨: æä¾›è¯­ä¹‰å¼•å¯¼,è®©é¢„æµ‹æ›´å‡†ç¡®
- Lossä»ç„¶æ˜¯æ ‡å‡†Flow Matching loss

### 2. ç±»ä¼¼ControlNet
```
ControlNet:
  condition_image â†’ ControlNet encoder â†’ features
  features + UNet_features â†’ UNet decoder â†’ prediction

Ours:
  semantic_image â†’ VAE encoder â†’ semantic_tokens
  semantic_tokens + x_t â†’ Cross-Attention â†’ x_t_enhanced
  x_t_enhanced â†’ Transformer â†’ prediction
```

### 3. æ¢¯åº¦æµåˆç†
- Cross-attentionå­¦ä¹ å¦‚ä½•ä»semanticæå–æœ‰ç”¨ä¿¡æ¯
- Transformer LoRAå­¦ä¹ å¦‚ä½•åˆ©ç”¨å¢å¼ºåçš„ç‰¹å¾
- ä¸¤è€…è”åˆä¼˜åŒ–,å®ç°ç«¯åˆ°ç«¯è®­ç»ƒ

---

## æ¨ç†æµç¨‹

æ¨ç†æ—¶,æµç¨‹ç±»ä¼¼:

```python
# 1. Encode inputs
visible_img = preprocess(input_image)
semantic_img = get_semantic_map(input_image)

# 2. Encode condition
diptych = cat([visible_img, zeros_like(visible_img)], dim=-1)
x_cond = encode_with_mask(diptych)

# 3. Encode semantic
semantic_tokens = encode_images(semantic_img)

# 4. Sampling loop
x_t = randn(...)  # åˆå§‹å™ªå£°
for t in reversed(timesteps):
    # Apply cross-attention
    x_t_enhanced = semantic_cross_attn(x_t, semantic_tokens)

    # Transformer forward
    hidden = cat([x_t_enhanced, x_cond])
    pred = transformer(hidden, t, ...)

    # Update x_t (Euleræˆ–å…¶ä»–sampler)
    x_t = update_step(x_t, pred, t)

# 5. Decode
output = vae.decode(x_t)
```

---

## å…³é”®æ£€æŸ¥ç‚¹

### å¯åŠ¨æ—¶åº”è¯¥çœ‹åˆ°:
```
[INFO] Semantic mode: cross_attention
[INFO] Using Cross-Attention with 1 layers, 8 heads
[INFO] Added 33088 semantic cross-attention parameters to optimizer
```

### Step 0-1 DEBUGè¾“å‡º:
```
[DEBUG] visible_img: [4, 3, 512, 512]
[DEBUG] target_img: [4, 3, 512, 512]
[DEBUG] semantic_img: [4, 3, 512, 512]
[DEBUG] diptych: [4, 3, 512, 1024]
[DEBUG] x_0: [4, 2048, 64]
[DEBUG] x_cond: [4, 2048, 320]
[DEBUG] semantic_tokens (encoded outside no_grad): [4, 2048, 64]

[DEBUG] Applying semantic cross-attention:
[DEBUG] x_t: [4, 2048, 64]
[DEBUG] semantic_tokens: [4, 2048, 64]
[DEBUG] Layer 0 scale: 0.000000

[DEBUG] x_t_enhanced: [4, 2048, 64]
[DEBUG] hidden_states (final): [4, 2048, 384]
```

### è®­ç»ƒä¸­(~100 steps):
```
[Step 100] Loss: 0.xxxx
[DEBUG] Layer 0 scale: 0.00x  # åº”è¯¥é€æ¸å¢å¤§
```

### è®­ç»ƒå(~1000 steps):
```
[DEBUG] Layer 0 scale: 0.1-0.5  # ç¨³å®šåœ¨æŸä¸ªå€¼
```

---

## æ€»ç»“

âœ… **å®ç°æ­£ç¡®æ€§**:
- æ¢¯åº¦æµç•…é€š (cross-attentionæœ‰æ¢¯åº¦)
- è®­ç»ƒç›®æ ‡ä¸€è‡´ (ä»ç„¶æ˜¯Flow Matching)
- æ¶æ„åˆç† (ç±»ä¼¼ControlNet)

âœ… **åˆ›æ–°æ€§**:
- æ˜¾å¼è¯­ä¹‰å¼•å¯¼ (vs ç®€å•fusion)
- å¯è§£é‡Š (attentionå¯è§†åŒ–)
- ç«¯åˆ°ç«¯ä¼˜åŒ–

âœ… **å®ç”¨æ€§**:
- è®­ç»ƒç¨³å®š (zero-init scale)
- å‚æ•°é‡å° (~33K)
- æ¨ç†ç®€å• (ç›´æ¥å¤ç”¨)

ç°åœ¨å¯ä»¥æ”¾å¿ƒè®­ç»ƒäº†! ğŸš€
