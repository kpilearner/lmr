# ğŸ¯ ControlNetå¼è¯­ä¹‰æ¡ä»¶æ³¨å…¥æ–¹æ¡ˆ

## é—®é¢˜åˆ†æ

**å½“å‰æ–¹æ¡ˆçš„å±€é™**:
- âŒ åƒç´ çº§ç®€å•å åŠ ,è¯­ä¹‰ä¿¡æ¯è¢«å¼±åŒ–
- âŒ æ²¡æœ‰æ˜¾å¼çš„è¯­ä¹‰å¼•å¯¼æœºåˆ¶
- âŒ ç¼ºä¹åˆ›æ–°æ€§

**ç›®æ ‡**:
- âœ… è¯­ä¹‰å›¾ä½œä¸ºç‹¬ç«‹æ§åˆ¶ä¿¡å·
- âœ… æ˜¾å¼çº¦æŸç”Ÿæˆ,é¿å…å¹»è§‰çƒ­ç›®æ ‡
- âœ… å¯å‘è®ºæ–‡çš„åˆ›æ–°ç‚¹

---

## æ–¹æ¡ˆA: Zero-Convolution Adapter (è½»é‡çº§)

### æ¶æ„
```
Semantic Image [B, 3, 512, 512]
    â†“
  VAE Encoder
    â†“
Semantic Latents [B, 16, 64, 64]
    â†“
  Zero-Conv (åˆå§‹åŒ–ä¸º0)
    â†“
Semantic Features [B, 64, 2048]
    â†“
  Add to x_cond
    â†“
FLUX Transformer
```

### å®ç°è¦ç‚¹
1. **å¤ç”¨FLUXçš„VAE** ç¼–ç semantic image
2. **Zero-initialized convolution** ç¡®ä¿åˆå§‹è®­ç»ƒç¨³å®š
3. **ç›´æ¥åŠ åˆ°x_cond** æœ€å°ä¾µå…¥æ€§ä¿®æ”¹

### ä¼˜ç‚¹
- å®ç°ç®€å•,è®­ç»ƒç¨³å®š
- LoRAä»ç„¶é€‚ç”¨
- ç±»ä¼¼ControlNetçš„zero-convæ€æƒ³

### ç¼ºç‚¹
- åˆ›æ–°æ€§ä¸€èˆ¬
- æ§åˆ¶åŠ›åº¦å¯èƒ½ä¸å¤Ÿå¼º

---

## æ–¹æ¡ˆB: Cross-Attention Injection (æ¨èâ­)

### æ¶æ„
```
Semantic Image [B, 3, 512, 512]
    â†“
  Semantic Encoder (LoRAå¾®è°ƒ)
    â†“
Semantic Tokens [B, 2048, 64]
    â†“
  Linear Projection
    â†“
Semantic Keys/Values [B, 2048, dim]
    â†“
  Cross-Attention in Transformer Blocks
    â†“
  Query: image features
  Key/Value: semantic features
    â†“
Controlled Generation
```

### å®ç°è¦ç‚¹
1. **ç‹¬ç«‹çš„semantic encoder**
   - ä½¿ç”¨FLUX VAE + projection layer
   - æˆ–è½»é‡çº§CNN encoder

2. **æ³¨å…¥åˆ°transformer blocks**
   - åœ¨æ¯ä¸ªdouble/single blockåæ·»åŠ cross-attention
   - Queryæ¥è‡ªimage,K/Væ¥è‡ªsemantic

3. **å¯å­¦ä¹ çš„scaleå‚æ•°**
   - æ§åˆ¶semanticå½±å“å¼ºåº¦
   - åˆå§‹åŒ–ä¸ºå°å€¼,é€æ¸å­¦ä¹ 

### ä¼˜ç‚¹
- âœ… **å¼ºåˆ›æ–°æ€§** - æ˜¾å¼çš„è¯­ä¹‰å¼•å¯¼æœºåˆ¶
- âœ… **å¯è§£é‡Šæ€§** - attentionå¯è§†åŒ–
- âœ… **æ§åˆ¶ç²¾ç¡®** - ä¸åŒåŒºåŸŸä¸åŒçº¦æŸ
- âœ… **è®ºæ–‡å–ç‚¹** - "Semantic-Guided Infrared Generation via Cross-Attention"

### ç¼ºç‚¹
- å®ç°å¤æ‚åº¦ä¸­ç­‰
- éœ€è¦ä¿®æ”¹transformer forward
- è®­ç»ƒæˆæœ¬ç¨é«˜

---

## æ–¹æ¡ˆC: Adapter Modules (æœ€çµæ´»)

### æ¶æ„
```
FLUX Transformer Block
    â†“
  Original Output
    â†“
  Semantic Adapter:
    - Self-Attention on semantic features
    - Cross-Attention with image features
    - FFN
    â†“
  Residual Add (with learnable gate)
    â†“
  Next Block
```

### å®ç°è¦ç‚¹
1. **åœ¨transformer blocksä¹‹é—´æ’å…¥adapter**
2. **Adapterç»“æ„**:
   - Semantic self-attention
   - Image-semantic cross-attention
   - Feed-forward
3. **Gated residual**: `output = image_feat + gate * adapter(semantic_feat)`

### ä¼˜ç‚¹
- æœ€çµæ´»,å¯ä»¥ç²¾ç»†æ§åˆ¶
- ä¸ç ´ååŸå§‹transformerç»“æ„
- è®­ç»ƒæ—¶å¯ä»¥freezeä¸»å¹²

### ç¼ºç‚¹
- å‚æ•°é‡è¾ƒå¤§
- å®ç°æœ€å¤æ‚

---

## æ¨èæ–¹æ¡ˆ: Cross-Attention (æ–¹æ¡ˆB)

### ä¸ºä»€ä¹ˆ?
1. **å¹³è¡¡æ€§å¥½** - æ•ˆæœå¼ºä½†ä¸è¿‡äºå¤æ‚
2. **åˆ›æ–°æ€§è¶³** - å¯å‘è®ºæ–‡
3. **LoRAå…¼å®¹** - å¯ä»¥åªè®­ç»ƒcross-attn + LoRA
4. **å¯è§£é‡Š** - å¯è§†åŒ–attentionçœ‹è¯­ä¹‰å¼•å¯¼

### å®ç°æ­¥éª¤

#### Step 1: ä¿®æ”¹transformer forward
```python
# åœ¨æ¯ä¸ªtransformer blockåæ·»åŠ semantic cross-attention
class SemanticCrossAttention(nn.Module):
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(dim, num_heads)
        self.norm = nn.LayerNorm(dim)
        self.scale = nn.Parameter(torch.zeros(1))  # åˆå§‹åŒ–ä¸º0

    def forward(self, image_feat, semantic_feat):
        # image_feat: [B, seq_len, dim]
        # semantic_feat: [B, sem_seq_len, dim]
        attn_out = self.cross_attn(
            query=image_feat,
            key=semantic_feat,
            value=semantic_feat
        )[0]
        return image_feat + self.scale * self.norm(attn_out)
```

#### Step 2: Semantic Encoder
```python
# å¤ç”¨FLUX VAEç¼–ç semantic
def encode_semantic(semantic_img):
    latents = vae.encode(semantic_img)
    tokens = pack_latents(latents)  # [B, 2048, 64]
    return tokens
```

#### Step 3: æ³¨å…¥åˆ°model.py
```python
# Encode semantic
semantic_tokens = encode_semantic(semantic_img)

# FLUX forward with semantic
transformer_out = self.transformer(
    hidden_states=cat(x_t, x_cond),
    semantic_condition=semantic_tokens,  # æ–°å¢
    ...
)
```

---

## è®­ç»ƒç­–ç•¥

### é˜¶æ®µ1: Warmup (å‰1k steps)
- Freeze FLUX backbone
- åªè®­ç»ƒsemantic cross-attention
- å­¦ä¹ åŸºæœ¬çš„è¯­ä¹‰å¯¹é½

### é˜¶æ®µ2: Joint Training
- FLUX LoRA + Semantic Cross-Attention åŒæ—¶è®­ç»ƒ
- æ›´ç»†ç²’åº¦çš„è¯­ä¹‰å¼•å¯¼

### é˜¶æ®µ3: Fine-tune (å¯é€‰)
- è°ƒæ•´scaleå‚æ•°
- å¹³è¡¡è¯­ä¹‰çº¦æŸå¼ºåº¦

---

## è®ºæ–‡å–ç‚¹

### Title
"Semantic-Guided Infrared Image Generation via Cross-Attention Conditioning"

### è´¡çŒ®ç‚¹
1. **Novel Architecture** - Cross-attention based semantic injection
2. **Explicit Control** - Prevents hallucination via semantic constraints
3. **Interpretable** - Attention visualization shows semantic guidance
4. **Effective** - å®éªŒè¯æ˜ä¼˜äºç®€å•fusion

---

## ä¸‹ä¸€æ­¥

ä½ æƒ³å®ç°æ–¹æ¡ˆB (Cross-Attention)å—?

æˆ‘å¯ä»¥:
1. âœ… ä¿®æ”¹transformer.pyæ·»åŠ cross-attention
2. âœ… ä¿®æ”¹model.pyæ·»åŠ semantic encoding
3. âœ… åˆ›å»ºè®­ç»ƒé…ç½®å’Œè„šæœ¬
4. âœ… å®ç°å¯è§†åŒ–å·¥å…·

**éœ€è¦æˆ‘å¼€å§‹å®ç°å—?** ğŸš€
